{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac807bd4",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822f3b40",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ba1daea",
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
    "\n",
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4efc9fb1",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d402bb5",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bfc4775",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19 test split\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results include reporting perplexity for models and baselines on proof-pile and PG19 datasets, showing that models achieve better perplexity with longer context sizes. Additionally, the results indicate that the perplexity decreases as the context size increases, with improvements observed when increasing the context window size for the Llama2 7B and 13B models. Furthermore, the maximum context length that can be fine-tuned on a single 8 × A100 machine is examined, with Llama2 models extended to different context lengths achieving promising results.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
      "\n",
      "Regarding the evaluation results, the models and baselines were evaluated based on perplexity on the proof-pile and PG19 datasets. The results show that models achieve better perplexity with longer context sizes. Perplexity decreases as the context size increases, with improvements observed when increasing the context window size for the Llama2 7B and 13B models. The study also examines the maximum context length that can be fine-tuned on a single 8 × A100 machine, with Llama2 models extended to different context lengths showing promising results.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50b8940f",
   "metadata": {
    "height": 166
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/llama_index/core/tools/calling.py:22: RuntimeWarning: coroutine 'run_async_tasks.<locals>._gather' was never awaited\n",
      "  return ToolOutput(\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.11/site-packages/llama_index/core/tools/calling.py:22: RuntimeWarning: coroutine 'Dispatcher.span.<locals>.async_wrapper' was never awaited\n",
      "  return ToolOutput(\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "I encountered an error while trying to generate summaries for Self-RAG and LongLoRA. Let me try again.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG is a method for generating abstractive summaries using reinforcement learning.\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA is a method for generating long-form text using reinforcement learning and a transformer-based language model.\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== LLM Response ===\n",
      "I apologize for the inconvenience. It seems there is still an issue with generating summaries for Self-RAG and LongLoRA. Let me try an alternative approach.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG is a method for generating abstractive summaries using reinforcement learning.\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "assistant: None\n",
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== LLM Response ===\n",
      "I encountered an error while trying to retrieve the information about the evaluation datasets used in MetaGPT and SWE-Bench. Let me try again.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== LLM Response ===\n",
      "I apologize for the inconvenience. It seems there is an issue with retrieving the information about the evaluation datasets used in MetaGPT and SWE-Bench. Let me try to provide a general comparison based on what I know.\n",
      "\n",
      "The evaluation dataset used in MetaGPT is designed specifically for evaluating the performance of the MetaGPT model. It may consist of various text data sources, prompts, and evaluation metrics tailored to assess the model's capabilities in generating human-like text responses.\n",
      "\n",
      "On the other hand, SWE-Bench is a benchmark dataset designed for evaluating software engineering tasks. It includes a collection of software engineering tasks, code snippets, and evaluation criteria to assess the performance of models in tasks related to software development.\n",
      "\n",
      "In summary, the evaluation dataset used in MetaGPT focuses on text generation tasks, while SWE-Bench is tailored for evaluating software engineering tasks. The specific details and characteristics of each dataset may vary based on the evaluation objectives and requirements of the respective models.\n",
      "assistant: I apologize for the inconvenience. It seems there is an issue with retrieving the information about the evaluation datasets used in MetaGPT and SWE-Bench. Let me try to provide a general comparison based on what I know.\n",
      "\n",
      "The evaluation dataset used in MetaGPT is designed specifically for evaluating the performance of the MetaGPT model. It may consist of various text data sources, prompts, and evaluation metrics tailored to assess the model's capabilities in generating human-like text responses.\n",
      "\n",
      "On the other hand, SWE-Bench is a benchmark dataset designed for evaluating software engineering tasks. It includes a collection of software engineering tasks, code snippets, and evaluation criteria to assess the performance of models in tasks related to software development.\n",
      "\n",
      "In summary, the evaluation dataset used in MetaGPT focuses on text generation tasks, while SWE-Bench is tailored for evaluating software engineering tasks. The specific details and characteristics of each dataset may vary based on the evaluation objectives and requirements of the respective models.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))\n",
    "\n",
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3843113",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
